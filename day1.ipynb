{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c9e8d6",
   "metadata": {},
   "source": [
    "# Tiny virsion of Building mini LLM By using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9df63480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.8.0-cp313-cp313-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\sachin\\import-export-agent\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\sachin\\import-export-agent\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in d:\\sachin\\import-export-agent\\.venv\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\sachin\\import-export-agent\\.venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\sachin\\import-export-agent\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached torch-2.8.0-cp313-cp313-win_amd64.whl (241.3 MB)\n",
      "Using cached torchvision-0.23.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.8.0-cp313-cp313-win_amd64.whl (2.5 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae35d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   #Neural network modules and layers\n",
    "import torch.optim as optim  #Optimization algorithms\n",
    "import torch.nn.functional as F #Functional interface for activation functions, loss functions, etc.\n",
    "from torch.utils.data import DataLoader, Dataset  #Data loading and batching\n",
    "import torchvision.transforms as transforms  #Image transformations and augmentations\n",
    "import torchvision.datasets as datasets  #Standard datasets like MNIST, CIFAR-10, etc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd565ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3428\n",
      "Epoch 1, Loss: 2.0677\n",
      "Epoch 2, Loss: 1.8238\n",
      "Epoch 3, Loss: 1.6055\n",
      "Epoch 4, Loss: 1.4086\n",
      "Epoch 5, Loss: 1.2322\n",
      "Epoch 6, Loss: 1.0709\n",
      "Epoch 7, Loss: 0.9263\n",
      "Epoch 8, Loss: 0.7960\n",
      "Epoch 9, Loss: 0.6784\n",
      "Epoch 10, Loss: 0.5735\n",
      "Epoch 11, Loss: 0.4813\n",
      "Epoch 12, Loss: 0.4011\n",
      "Epoch 13, Loss: 0.3320\n",
      "Epoch 14, Loss: 0.2733\n",
      "Epoch 15, Loss: 0.2240\n",
      "Epoch 16, Loss: 0.1829\n",
      "Epoch 17, Loss: 0.1492\n",
      "Epoch 18, Loss: 0.1216\n",
      "Epoch 19, Loss: 0.0993\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Create data (torch)\n",
    "x_train = torch.randn(100, 784)  # 100 samples, 784 features\n",
    "y_train = torch.randint(0, 10, (100,))  # 100 labels (0-9)\n",
    "\n",
    "# 2. Define model (nn)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # F for activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# 3. Define loss and optimizer (nn + optim)\n",
    "criterion = nn.CrossEntropyLoss()  # or F.cross_entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Training loop (torch + optim)\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)  # Compute loss\n",
    "    loss.backward()                     # torch computes gradients\n",
    "    optimizer.step()                    # optim updates parameters\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20555b32",
   "metadata": {},
   "source": [
    "# Here we will start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d9e74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laibraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686ef85",
   "metadata": {},
   "source": [
    "## Step 2. Prepare a Small Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81310c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Artificial intelligence is the future of technology and innovation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680bf46",
   "metadata": {},
   "source": [
    "## Step 3. Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c9f9b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "749c2ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'and',\n",
       " 'future',\n",
       " 'innovation',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'the']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(words))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5cd98f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fc3fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Artificial': 0, 'and': 1, 'future': 2, 'innovation': 3, 'intelligence': 4, 'is': 5, 'of': 6, 'technology': 7, 'the': 8}\n"
     ]
    }
   ],
   "source": [
    "#index mapping\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21a6f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Artificial', 1: 'and', 2: 'future', 3: 'innovation', 4: 'intelligence', 5: 'is', 6: 'of', 7: 'technology', 8: 'the'}\n"
     ]
    }
   ],
   "source": [
    "ix_to_word = {i: word for word,i in word_to_ix.items()}\n",
    "print(ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12bdbb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'Artificial': 0, 'and': 1, 'future': 2, 'innovation': 3, 'intelligence': 4, 'is': 5, 'of': 6, 'technology': 7, 'the': 8}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93851100",
   "metadata": {},
   "source": [
    "## Step 4. Create Training Data (Next-Word Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7fd0caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Artificial', 'intelligence', 'is'], 'the'), (['intelligence', 'is', 'the'], 'future'), (['is', 'the', 'future'], 'of')]\n"
     ]
    }
   ],
   "source": [
    "def make_data(words, context_size=3):\n",
    "    data = []\n",
    "    for i in range(len(words) - context_size):\n",
    "        context = words[i : i + context_size]  # ✅ take a slice, not a single index\n",
    "        target = words[i + context_size]       # next word to predict\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "data = make_data(words)\n",
    "print(data[:3])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1470d9",
   "metadata": {},
   "source": [
    "## Step 5. Define a Tiny Language Model (like GPT Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a3d9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_dim=32):\n",
    "        super(TinyLLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs).view(1, -1)\n",
    "        out = F.relu(self.fc1(embeds))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704079f0",
   "metadata": {},
   "source": [
    "## Step 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e60826b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 14.1975\n",
      "Epoch 50, Loss: 0.0022\n",
      "Epoch 100, Loss: 0.0007\n",
      "Epoch 150, Loss: 0.0004\n",
      "Epoch 200, Loss: 0.0002\n",
      "Epoch 250, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "model = TinyLLM(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "\n",
    "        # Forward + Backward\n",
    "        model.zero_grad()\n",
    "        logits = model(context_idxs)\n",
    "        loss = loss_fn(logits, target_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696372b",
   "metadata": {},
   "source": [
    "## Step 7. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f054dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: intelligence is the → Predicted next word: future\n"
     ]
    }
   ],
   "source": [
    "def predict_next(context_words):\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context_words], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(context_idxs)\n",
    "        predicted_idx = torch.argmax(logits, dim=1).item()\n",
    "        return ix_to_word[predicted_idx]\n",
    "\n",
    "context = [\"intelligence\", \"is\", \"the\"]\n",
    "next_word = predict_next(context)\n",
    "print(f\"Input: {' '.join(context)} → Predicted next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81294c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
